<?xml version="1.0"?>
<!-- DO NOT CHANGE THIS FILE MANUALLY, IT IS AUTOMATICALLY GENERATED -->
<!-- GENERATED FROM https://github.com/linuxserver/docker-faster-whisper/blob/main/readme-vars.yml -->
<Container version="2">
    <Name>faster-whisper</Name>
    <Repository>lscr.io/linuxserver/faster-whisper</Repository>
    <Registry>https://github.com/orgs/linuxserver/packages/container/package/faster-whisper</Registry>
    <DonateText>Donations</DonateText>
    <DonateLink>https://www.linuxserver.io/donate</DonateLink>
    <DonateImg>https://raw.githubusercontent.com/linuxserver/docker-templates/master/linuxserver.io/img/donate.png</DonateImg>
    <Network>bridge</Network>
    <Privileged>false</Privileged>
    <Support>https://github.com/linuxserver/docker-faster-whisper/issues/new/choose</Support>
    <Shell>bash</Shell>
    <GitHub>https://github.com/linuxserver/docker-faster-whisper#application-setup</GitHub>
    <ReadMe>https://github.com/linuxserver/docker-faster-whisper#readme</ReadMe>
    <Branch>
        <Tag>latest</Tag>
        <TagDescription>Stable releases</TagDescription>
    </Branch>
    <Branch>
        <Tag>gpu</Tag>
        <TagDescription>Releases with Nvidia GPU support (amd64 only)</TagDescription>
        <ReadMe>https://github.com/linuxserver/docker-faster-whisper/tree/gpu#readme</ReadMe>
        <GitHub>https://github.com/linuxserver/docker-faster-whisper/tree/gpu#application-setup</GitHub>
    </Branch>
    <Project>https://github.com/SYSTRAN/faster-whisper</Project>
    <Overview>Faster-whisper(https://github.com/SYSTRAN/faster-whisper) is a reimplementation of OpenAI&#39;s Whisper model using CTranslate2, which is a fast inference engine for Transformer models. This container provides a Wyoming protocol server for faster-whisper.</Overview>
    <TemplateURL>https://raw.githubusercontent.com/linuxserver/templates/main/unraid/faster-whisper.xml</TemplateURL>
    <Icon>https://raw.githubusercontent.com/linuxserver/docker-templates/master/linuxserver.io/img/linuxserver-ls-logo.png</Icon>
    <Date>2024-12-30</Date>
    <Changes>
### 2024-12-30
- Add arm64 support for non-GPU builds.

### 2024-12-05
- Build from Github releases rather than Pypi.

### 2024-07-18
- Rebase to Ubuntu Noble.

### 2024-05-19
- Bump CUDA to 12 on GPU branch.

### 2024-01-08
- Add GPU branch.

### 2023-11-25
- Initial Release.

    </Changes>
    <Config Name="WebUI" Target="10300" Default="10300" Mode="tcp" Description="Wyoming connection port." Type="Port" Display="always" Required="true" Mask="false"/>
    <Config Name="Appdata" Target="/config" Default="" Mode="rw" Description="Local path for Whisper config files." Type="Path" Display="advanced" Required="true" Mask="false"/>
    <Config Name="WHISPER_MODEL" Target="WHISPER_MODEL" Default="tiny-int8" Description="Whisper model that will be used for transcription. From here(https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/utils.py#L12-L31), all with `-int8` compressed variants" Type="Variable" Display="always" Required="true" Mask="false"/>
    <Config Name="WHISPER_BEAM" Target="WHISPER_BEAM" Default="1" Description="Number of candidates to consider simultaneously during transcription." Type="Variable" Display="always" Required="false" Mask="false"/>
    <Config Name="WHISPER_LANG" Target="WHISPER_LANG" Default="en" Description="Language that you will speak to the add-on." Type="Variable" Display="always" Required="false" Mask="false"/>
    <Config Name="PUID" Target="PUID" Default="99" Description="Container Variable: PUID" Type="Variable" Display="advanced" Required="true" Mask="false"/>
    <Config Name="PGID" Target="PGID" Default="100" Description="Container Variable: PGID" Type="Variable" Display="advanced" Required="true" Mask="false"/>
    <Config Name="UMASK" Target="UMASK" Default="022" Description="Container Variable: UMASK" Type="Variable" Display="advanced" Required="false" Mask="false"/>
</Container>
